{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7abe4d4",
   "metadata": {},
   "source": [
    "## Build a fake or not table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4bc9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sign_utils import DTW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compare(win_size=0.5):\n",
    "    dissim = []\n",
    "    writers = pd.read_csv(\"./writers.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "    features = [\"x\", \"y\", \"pressure\", \"v_x\", \"v_y\"]\n",
    "\n",
    "    for writer in writers[0]:\n",
    "        writer = f\"{int(writer):03d}\"\n",
    "        print(\"writer\", writer)\n",
    "\n",
    "        # --- load ALL 5 genuine enrollment signatures as templates ---\n",
    "        enroll_seqs = []\n",
    "        for g in range(1, 6):\n",
    "            enr = pd.read_csv(f\"./enrollment/{writer}-g-{g:02d}.tsv\", sep=\"\\t\", header=0)\n",
    "            enr = enr[features].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "            # Normalize with z-score\n",
    "            enr_norm = (enr - enr.mean()) / (enr.std() + 1e-8)\n",
    "            enroll_seqs.append(enr_norm.values)\n",
    "\n",
    "        # --- verification: compute min DTW distance to any enrollment template ---\n",
    "        for i in range(1, 46):\n",
    "            idx = f\"{i:02d}\"\n",
    "            ver = pd.read_csv(f\"./verification/{writer}-{idx}.tsv\", sep=\"\\t\", header=0)\n",
    "            ver = ver[features].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "            # Normalize with z-score\n",
    "            ver_norm = (ver - ver.mean()) / (ver.std() + 1e-8)\n",
    "            seq2 = ver_norm.values\n",
    "\n",
    "            # Use mean of all DTW distances\n",
    "            score = np.mean([DTW(seq1, seq2, win_size) for seq1 in enroll_seqs])\n",
    "\n",
    "            dissim.append({\n",
    "                \"writer\": writer,\n",
    "                \"idx\": idx,\n",
    "                \"signature_id\": f\"{writer}-{idx}\",\n",
    "                \"dissim\": float(score),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(dissim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5815ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writer 001\n",
      "writer 002\n",
      "writer 003\n",
      "writer 004\n",
      "writer 005\n",
      "writer 006\n",
      "writer 007\n",
      "writer 008\n",
      "writer 009\n",
      "writer 010\n",
      "writer 011\n",
      "writer 012\n",
      "writer 013\n",
      "writer 014\n",
      "writer 015\n",
      "writer 016\n",
      "writer 017\n",
      "writer 018\n",
      "writer 019\n",
      "writer 020\n",
      "writer 021\n",
      "writer 022\n",
      "writer 023\n",
      "writer 024\n",
      "writer 025\n",
      "writer 026\n",
      "writer 027\n",
      "writer 028\n",
      "writer 029\n",
      "writer 030\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writer</th>\n",
       "      <th>idx</th>\n",
       "      <th>signature_id</th>\n",
       "      <th>dissim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>01</td>\n",
       "      <td>001-01</td>\n",
       "      <td>71.834969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001</td>\n",
       "      <td>02</td>\n",
       "      <td>001-02</td>\n",
       "      <td>68.827057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001</td>\n",
       "      <td>03</td>\n",
       "      <td>001-03</td>\n",
       "      <td>220.977055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001</td>\n",
       "      <td>04</td>\n",
       "      <td>001-04</td>\n",
       "      <td>367.496447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001</td>\n",
       "      <td>05</td>\n",
       "      <td>001-05</td>\n",
       "      <td>72.564685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  writer idx signature_id      dissim\n",
       "0    001  01       001-01   71.834969\n",
       "1    001  02       001-02   68.827057\n",
       "2    001  03       001-03  220.977055\n",
       "3    001  04       001-04  367.496447\n",
       "4    001  05       001-05   72.564685"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dissim = compare(win_size=0.15)\n",
    "dissim.to_csv(\"dissim.csv\", index=False)\n",
    "dissim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3b3a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissim.to_csv(\"dissim.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7f08879-524b-4071-8d5a-84cafea4ca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision (ranking on -dissim): 0.7155351922588026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import pandas as pd\n",
    "\n",
    "gt_df = pd.read_csv(\"gt.tsv\", sep=\"\\t\", header=None, names=[\"signature_id\", \"ground_truth\"])\n",
    "df = pd.merge(dissim, gt_df, on=\"signature_id\", how=\"inner\")\n",
    "\n",
    "y_true = df[\"ground_truth\"].map({\"genuine\": 1, \"forgery\": 0}).values\n",
    "y_score = (-df[\"dissim\"]).values  # IMPORTANT: higher score => more genuine\n",
    "\n",
    "ap = average_precision_score(y_true, y_score)\n",
    "print(\"Average Precision (ranking on -dissim):\", ap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9ca5e",
   "metadata": {},
   "source": [
    "## Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3b04ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def classify_sign(top_k):\n",
    "    # Load the dissimilarity CSV\n",
    "    df = pd.read_csv('dissim.csv')\n",
    "\n",
    "    # Assuming columns: writer_id, signature_id, dissimilarity\n",
    "    results = []\n",
    "\n",
    "    for writer, group in df.groupby('writer'):\n",
    "\n",
    "        writer_idx = f\"{writer:03d}\"\n",
    "        # Sort by dissimilarity (ascending)\n",
    "        sorted_group = group.sort_values('dissim')\n",
    "        # Top k as genuine\n",
    "        genuine = sorted_group.head(top_k)\n",
    "        # Rest as forgery\n",
    "        forgery = sorted_group.iloc[top_k:]\n",
    "        # Append results\n",
    "        results.extend([(writer_idx + \"-\" + str(int(row['idx'])), 'genuine') for _, row in genuine.iterrows()])\n",
    "        results.extend([(writer_idx + \"-\" + str(int(row['idx'])), 'forgery') for _, row in forgery.iterrows()])\n",
    "\n",
    "    # Save to CSV\n",
    "    output_df = pd.DataFrame(results, columns=['signature_id', 'classification'])\n",
    "    output_df.to_csv('classified_signatures.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d720f2eb",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fc36343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top K = 10\n",
      "Precision: 0.996\n",
      "Recall: 0.499\n",
      "Mean Average Precision (MAP): 0.721\n",
      "Top K = 15\n",
      "Precision: 0.992\n",
      "Recall: 0.743\n",
      "Mean Average Precision (MAP): 0.852\n",
      "Top K = 18\n",
      "Precision: 0.984\n",
      "Recall: 0.878\n",
      "Mean Average Precision (MAP): 0.918\n",
      "Top K = 20\n",
      "Precision: 0.973\n",
      "Recall: 0.965\n",
      "Mean Average Precision (MAP): 0.954\n",
      "Top K = 25\n",
      "Precision: 0.779\n",
      "Recall: 0.975\n",
      "Mean Average Precision (MAP): 0.770\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "\n",
    "k_list = [10, 15, 18, 20, 25]\n",
    "\n",
    "for k in k_list:\n",
    "\n",
    "    classify_sign(k)\n",
    "\n",
    "    # Load predictions\n",
    "    pred_df = pd.read_csv('classified_signatures.csv')\n",
    "\n",
    "    # Load ground truth\n",
    "    gt_df = pd.read_csv('gt.tsv', sep='\\t', names=['signature_id', 'ground_truth'])\n",
    "\n",
    "    # Merge on signature_id\n",
    "    merged = pd.merge(pred_df, gt_df, on='signature_id')\n",
    "\n",
    "    # Convert labels to binary (genuine=1, forgery=0)\n",
    "    y_true = merged['ground_truth'].map({'genuine': 1, 'forgery': 0})\n",
    "    y_pred = merged['classification'].map({'genuine': 1, 'forgery': 0})\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    map_score = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Top K = {k}\")\n",
    "    print(f'Precision: {precision:.3f}')\n",
    "    print(f'Recall: {recall:.3f}')\n",
    "    print(f'Mean Average Precision (MAP): {map_score:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
